select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
{ makeX(
train = .,
contrasts.arg = lapply(., function(x) contrasts(x, contrasts = FALSE))
)
} %>%
as.data.frame()
#combine one-hot & scaled test
X_test_processed <- cbind(test[numeric_columns], test_categorical)
dim(X_train_processed)
dim(X_test_processed)
dim(Y_train)
dim(Y_test)
processed_train_data <- data.frame(cbind(X_train_processed, Y_train))
processed_test_data <- data.frame(cbind(X_test_processed, Y_test))
X_train_processed <- as.matrix(X_train_processed)
Y_train <- as.matrix(Y_train)
X_test_processed <- as.matrix(X_test_processed)
Y_test <- as.matrix(Y_test)
hyperparams <- expand.grid(
num_layer = c(1),
num_nodes = c(1),
num_dropout = c(0.2, 0.3, 0.4),
num_lr = c(0.0005, 0.001),
batch_size = c(16),
num_l2 = c(0.2,0.4,0.6)
)
# Perform K-Fold Cross-Validation for Grid Search
k_folds <- 5
folds <- sample(rep(1:k_folds, length.out = nrow(processed_train_data)))
# Grid Search with Cross-Validation
results <- data.frame(hyperparams)
results$Mean_C_Index <- NA
results$Std_C_Index <- NA
for (i in 1:nrow(hyperparams)) {
params <- as.list(hyperparams[i, ])
c_index_scores <- c()
for (fold in 1:k_folds) {
# Train/Validation Split
train_indices <- which(folds != fold)
val_indices <- which(folds == fold)
X_train_fold <- X_train_processed[train_indices, ]
Y_train_fold <- Y_train[train_indices, ]
X_val_fold <- X_train_processed[val_indices, ]
Y_val_fold <- Y_train[val_indices, ]
# Build and Train Model
deepsurv_model <- build_deepsurv(
num_input = ncol(X_train_fold),
num_layer = params$num_layer,
num_nodes = params$num_nodes,
string_activation = "selu",
num_l2 = params$num_l2,
num_dropout = params$num_dropout,
num_lr = params$num_lr,
lr_decay = 1e-2
)
# Add an early stopping callback
early_stopping <- callback_early_stopping(
monitor = "val_loss",       # Monitor validation loss
patience = 20,              # Number of epochs with no improvement before stopping
restore_best_weights = TRUE # Restore weights from the best epoch
)
history <- deepsurv_model %>% fit(
x = X_train_fold,
y = Y_train_fold,
batch_size = params$batch_size,
epochs = 100,
validation_data = list(X_val_fold, Y_val_fold),
verbose = 0,
callbacks = list(early_stopping)
)
# Evaluate on Validation Set
predictions <- deepsurv_model %>% predict(X_val_fold)
c_index_scores <- c(c_index_scores, c_index(predictions, Y_val_fold[, 2], Y_val_fold[, 1]))
}
# Store Mean and Standard Deviation of C-Index Across Folds
results$Mean_C_Index[i] <- mean(c_index_scores, na.rm = TRUE)
results$Std_C_Index[i] <- sd(c_index_scores, na.rm = TRUE)
}
# Select Best Hyperparameters
best_params <- results[which.max(results$Mean_C_Index), ]
print(best_params)
# Train the final model with the best parameters
cat("Training final model with best hyperparameters...\n")
final_model <- build_deepsurv(
num_input = ncol(X_train_processed),
num_layer = best_params$num_layer,
num_nodes = best_params$num_nodes,
string_activation = "selu",
num_l2 = best_params$num_l2,
num_dropout = best_params$num_dropout,
num_lr = best_params$num_lr,
lr_decay = 1e-2)
print(final_model)
# Train the model using the entire training dataset
final_history <- final_model %>% fit(
x = X_train_processed,
y = Y_train,
batch_size = best_params$batch_size,
epochs = 100,
validation_split = 0.25,  # Keep 25% of the training data for internal validation
verbose = 1,
callbacks =list(
callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 5, min_lr = 1e-6),
callback_early_stopping(monitor = "loss", patience = 10, restore_best_weights = TRUE)
)
)
# Evaluate on the training set
cat("Evaluating final model on training set...\n")
final_train_predictions <- final_model %>% predict(X_train_processed)
final_train_c_index <- c_index(
final_train_predictions,
Y_train[, "pfs_status"],
Y_train[, "pfs_months"]
)
cat("Final Train C-Index:", final_train_c_index, "\n")
# Evaluate on the test set
cat("Evaluating final model on test set...\n")
final_test_predictions <- final_model %>% predict(X_test_processed)
final_test_c_index <- c_index(
final_test_predictions,
Y_test[, "pfs_status"],
Y_test[, "pfs_months"]
)
cat("Final Test C-Index:", final_test_c_index, "\n")
# Save the results
results_summary <- data.frame(
Metric = c("Train C-Index", "Test C-Index"),
Value = c(final_train_c_index, final_test_c_index)
)
valid_results <- subset(results, !is.na(Std_C_Index))
sorted_results <- valid_results[order(-valid_results$Mean_C_Index, valid_results$Std_C_Index), ]
# Fetch the top row
best_result <- sorted_results[1, ]
# Print the best hyperparameter set
cat("Best hyperparameter set with highest Mean_C_Index and lowest Std_C_Index:\n")
cat("Parameter set with low variability across folds:\n")
print(best_result)
#variable importance
variable_importance <- get_variable_importance(final_model, X_train_processed)
print(variable_importance)
##Plot Feature Attribution
importance_df <- variable_importance[order(variable_importance$Importance, decreasing = T), ]
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue", color = "black", alpha = 0.7) +
coord_flip() +  # Flip axes to make feature names readable
labs(title = "Clinical and All Genomic Features Attribution in DeepSurv Model",
x = "Features",
y = "Importance Score") +
scale_fill_brewer(palette = "Set1") +
theme(
text = element_text(size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) + theme_minimal()
#===Top feature from GBFA PLOT==
median_sensitivity <- round(median(importance_df$Importance), 4)
shortlisted_GBFA <- importance_df[importance_df$Importance > median_sensitivity,]
shortlisted_GBFA
# Set your Working Directory and load R source code of dependent libraries
# install.packages("rstudioapi")
library(rstudioapi)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd()
source("pcm_libraries.R")
source("rsf_libraries.R")
source("deepsurv_libraries.R")
source("deepsurv.R")
#-----------------------> Load Data <----------------------------
#--- Read any of the data below to run the respective ML Experiment Pipeline--
# clinical <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")%>%
#   select(age:pfs_status)
clinical_all_genomic <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")
# clinical_novel_genomic <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")%>%
#   select(-c(fat3_diff, atm_diff, kmt2d_diff, foxa1_diff, tp53_diff, spop_diff,
#             smad4_diff,lrp1b_diff,  idh1_diff, ctnnb1_diff, braf_diff, kmt2c_diff))
# clinical_known_genomic <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")%>%
#   select(-c(nkx3_1_diff, csmd3_diff, trrap_diff, chd4_diff, vwf_diff, ephb1_diff, herc2_diff, mcm3_diff,
#             spta1_diff, sall1_diff,  herc1_diff, rybp_diff, ttn_diff, chd5_diff, myh6_diff))
dat_new <- clinical_all_genomic  #----> update this based on the data options loaded
#----------------------->Data Modelling<-----------------
#---convert integer to numeric and character to categorical-
integer_cols <- sapply(dat_new, is.integer)
dat_new[integer_cols] <- lapply(dat_new[integer_cols], as.numeric)
# Convert character columns to categorical
char_cols <- sapply(dat_new, is.character)
dat_new[char_cols] <- lapply(dat_new[char_cols], factor)
#---Arrange data---
endpoints <- dat_new %>%  #we need status be integer & months be  but numeric here
dplyr::select(pfs_months, pfs_status)
vars <- dat_new %>%
dplyr::select(-c(pfs_months, pfs_status))
pfs_status <- endpoints$pfs_status
pfs_months <- endpoints$pfs_months
#---- Partition Data ----
set.seed(1)
comb_dat <- cbind(vars, pfs_months, pfs_status)
sample = sample.split( Y= comb_dat$pfs_status, SplitRatio = 0.70)
train <- subset(comb_dat, sample == TRUE)
test  <- subset(comb_dat, sample == FALSE)
#---Train set --
X_train <- train %>% select(-c(pfs_months, pfs_status))
Y_train <- train %>% select(pfs_months, pfs_status)
#-- Test set --
X_test <- test %>% select(-c(pfs_months, pfs_status))
Y_test <- test %>% select(pfs_months, pfs_status)
#---Process Train Data---
#One-Hot Encode Categorical Columns
catego_columns <- sapply(X_train, is.factor)
categorical_columns <- colnames(X_train)[catego_columns]
train_categorical <- train %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~ ., data = .) %>%
as.data.frame()
num_columns_train <- sapply(X_train, is.numeric)
numeric_columns_train <- X_train[num_columns_train]
#combine onehot & scaled
X_train_processed <- cbind(numeric_columns_train, train_categorical)
#--- Process Test Data ---
# -- one-hot encode categorical
test_categorical <- test %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~., data = ., xlev = attr(train_categorical, "contrasts")) %>%
as.data.frame()
num_columns_test <- sapply(X_test, is.numeric)
numeric_columns_test <- X_test[num_columns_test]
#combine one-hot & scaled test
X_test_processed <- cbind(numeric_columns_test, test_categorical)
dim(X_train_processed)
dim(X_test_processed)
dim(Y_train)
dim(Y_test)
processed_train_data <- data.frame(cbind(X_train_processed, Y_train))
processed_test_data <- data.frame(cbind(X_test_processed, Y_test))
X_train_processed <- as.matrix(X_train_processed)
Y_train <- as.matrix(Y_train)
X_test_processed <- as.matrix(X_test_processed)
Y_test <- as.matrix(Y_test)
#================DeepSurv ===================================================
#===Cross Validation==
source("deepsurv_libraries.R")
source("deepsurv.R")
tensorflow::set_random_seed(1)
#Prep Data for Deepsurv Jinli ---> train & test are dataframes
#==Train set ==
X_train <- train %>% select(-c(pfs_months, pfs_status))
Y_train <- train %>% select(pfs_months, pfs_status)
#==Test set ==
X_test <- test %>% select(-c(pfs_months, pfs_status))
Y_test <- test %>% select(pfs_months, pfs_status)
#======== Process Training data ===========
num_columns <- sapply(X_train, is.numeric)
numeric_columns <- colnames(X_train)[num_columns]
#Min-Max Scaling parameters of training data
scale_params <- train %>%
select(all_of(numeric_columns)) %>%
summarise(across(everything(), list(min=min, max=max)))
#Apply Min-Max scaling on training data
X_train[numeric_columns] <- train %>%
select(all_of(numeric_columns)) %>%
mutate(across(everything(), ~ (. - scale_params[[paste0(cur_column(), "_min")]]) /
(scale_params[[paste0(cur_column(), "_max")]] - scale_params[[paste0(cur_column(),"_min")]])
))
#One-Hot Encode Categorical Columns
catego_columns <- sapply(X_train, is.factor)
categorical_columns <- colnames(X_train)[catego_columns]
#library(glmnet)
train_categorical <- train %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
{ makeX(
train = .,
contrasts.arg = lapply(., function(x) contrasts(x, contrasts = FALSE)))
} %>%as.data.frame()
#combine onehot & scaled
X_train_processed <- cbind(X_train[numeric_columns] , train_categorical)
#== Apply Transformation and process Test Data ===
#min-max scale on test data
X_test[numeric_columns] <- test %>%
select(all_of(numeric_columns)) %>%
mutate(across(everything(),
~ (. - scale_params[[paste0(cur_column(), "_min")]]) /
(scale_params[[paste0(cur_column(), "_max")]] - scale_params[[paste0(cur_column(), "_min")]])
))
#one-hot encode categorical
test_categorical <- test %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
{ makeX(
train = .,
contrasts.arg = lapply(., function(x) contrasts(x, contrasts = FALSE))
)
} %>%
as.data.frame()
#combine one-hot & scaled test
X_test_processed <- cbind(test[numeric_columns], test_categorical)
dim(X_train_processed)
dim(X_test_processed)
dim(Y_train)
dim(Y_test)
processed_train_data <- data.frame(cbind(X_train_processed, Y_train))
processed_test_data <- data.frame(cbind(X_test_processed, Y_test))
X_train_processed <- as.matrix(X_train_processed)
Y_train <- as.matrix(Y_train)
X_test_processed <- as.matrix(X_test_processed)
Y_test <- as.matrix(Y_test)
hyperparams <- expand.grid(
num_layer = c(1),
num_nodes = c(1),
num_dropout = c(0.2, 0.3, 0.4),
num_lr = c(0.0005, 0.001),
batch_size = c(16),
num_l2 = c(0.2,0.4,0.6)
)
# Perform K-Fold Cross-Validation for Grid Search
k_folds <- 5
folds <- sample(rep(1:k_folds, length.out = nrow(processed_train_data)))
# Grid Search with Cross-Validation
results <- data.frame(hyperparams)
results$Mean_C_Index <- NA
results$Std_C_Index <- NA
for (i in 1:nrow(hyperparams)) {
params <- as.list(hyperparams[i, ])
c_index_scores <- c()
for (fold in 1:k_folds) {
# Train/Validation Split
train_indices <- which(folds != fold)
val_indices <- which(folds == fold)
X_train_fold <- X_train_processed[train_indices, ]
Y_train_fold <- Y_train[train_indices, ]
X_val_fold <- X_train_processed[val_indices, ]
Y_val_fold <- Y_train[val_indices, ]
# Build and Train Model
deepsurv_model <- build_deepsurv(
num_input = ncol(X_train_fold),
num_layer = params$num_layer,
num_nodes = params$num_nodes,
string_activation = "selu",
num_l2 = params$num_l2,
num_dropout = params$num_dropout,
num_lr = params$num_lr,
lr_decay = 1e-2
)
# Add an early stopping callback
early_stopping <- callback_early_stopping(
monitor = "val_loss",       # Monitor validation loss
patience = 20,              # Number of epochs with no improvement before stopping
restore_best_weights = TRUE # Restore weights from the best epoch
)
history <- deepsurv_model %>% fit(
x = X_train_fold,
y = Y_train_fold,
batch_size = params$batch_size,
epochs = 100,
validation_data = list(X_val_fold, Y_val_fold),
verbose = 0,
callbacks = list(early_stopping)
)
# Evaluate on Validation Set
predictions <- deepsurv_model %>% predict(X_val_fold)
c_index_scores <- c(c_index_scores, c_index(predictions, Y_val_fold[, 2], Y_val_fold[, 1]))
}
# Store Mean and Standard Deviation of C-Index Across Folds
results$Mean_C_Index[i] <- mean(c_index_scores, na.rm = TRUE)
results$Std_C_Index[i] <- sd(c_index_scores, na.rm = TRUE)
}
# Select Best Hyperparameters
best_params <- results[which.max(results$Mean_C_Index), ]
print(best_params)
# Train the final model with the best parameters
cat("Training final model with best hyperparameters...\n")
final_model <- build_deepsurv(
num_input = ncol(X_train_processed),
num_layer = best_params$num_layer,
num_nodes = best_params$num_nodes,
string_activation = "selu",
num_l2 = best_params$num_l2,
num_dropout = best_params$num_dropout,
num_lr = best_params$num_lr,
lr_decay = 1e-2)
print(final_model)
# Train the model using the entire training dataset
final_history <- final_model %>% fit(
x = X_train_processed,
y = Y_train,
batch_size = best_params$batch_size,
epochs = 100,
validation_split = 0.25,  # Keep 25% of the training data for internal validation
verbose = 1,
callbacks =list(
callback_reduce_lr_on_plateau(monitor = "loss", factor = 0.5, patience = 5, min_lr = 1e-6),
callback_early_stopping(monitor = "loss", patience = 10, restore_best_weights = TRUE)
)
)
# Evaluate on the training set
cat("Evaluating final model on training set...\n")
final_train_predictions <- final_model %>% predict(X_train_processed)
final_train_c_index <- c_index(
final_train_predictions,
Y_train[, "pfs_status"],
Y_train[, "pfs_months"]
)
cat("Final Train C-Index:", final_train_c_index, "\n")
# Evaluate on the test set
cat("Evaluating final model on test set...\n")
final_test_predictions <- final_model %>% predict(X_test_processed)
final_test_c_index <- c_index(
final_test_predictions,
Y_test[, "pfs_status"],
Y_test[, "pfs_months"]
)
cat("Final Test C-Index:", final_test_c_index, "\n")
# Evaluate on the test set
cat("Evaluating final model on test set...\n")
final_test_predictions <- final_model %>% predict(X_test_processed)
final_test_c_index <- c_index(
final_test_predictions,
Y_test[, "pfs_status"],
Y_test[, "pfs_months"]
)
cat("Final Test C-Index:", final_test_c_index, "\n")
# Save the results
results_summary <- data.frame(
Metric = c("Train C-Index", "Test C-Index"),
Value = c(final_train_c_index, final_test_c_index)
)
valid_results <- subset(results, !is.na(Std_C_Index))
sorted_results <- valid_results[order(-valid_results$Mean_C_Index, valid_results$Std_C_Index), ]
# Fetch the top row
best_result <- sorted_results[1, ]
# Print the best hyperparameter set
cat("Best hyperparameter set with highest Mean_C_Index and lowest Std_C_Index:\n")
cat("Parameter set with low variability across folds:\n")
print(best_result)
#variable importance
variable_importance <- get_variable_importance(final_model, X_train_processed)
print(variable_importance)
##Plot Feature Attribution
importance_df <- variable_importance[order(variable_importance$Importance, decreasing = T), ]
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "steelblue", color = "black", alpha = 0.7) +
coord_flip() +  # Flip axes to make feature names readable
labs(title = "Clinical and All Genomic Features Attribution in DeepSurv Model",
x = "Features",
y = "Importance Score") +
scale_fill_brewer(palette = "Set1") +
theme(
text = element_text(size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) + theme_minimal()
#===Top feature from GBFA PLOT==
median_sensitivity <- round(median(importance_df$Importance), 4)
shortlisted_GBFA <- importance_df[importance_df$Importance > median_sensitivity,]
shortlisted_GBFA
# Plot histogram of shortlisted importance
ggplot(shortlisted_GBFA, aes(x = reorder(Feature, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "skyblue", color = "black", alpha = 0.7) +
coord_flip() +  # Flip axes to make feature names readable
labs(title = "Clinical Features Attribution in DeepSurv Model above Median",
x = "Features",
y = "Importance Score") +
scale_fill_brewer(palette = "Set1") +
theme(
text = element_text(size = 10),
plot.title = element_text(hjust = 0.5, size = 16, face = "bold")
) + theme_minimal()
#======Deepsurv Risk & Survival Predictions=======
median_sensitivity <- round(median(importance_df$Importance), 4)
shortlisted <- importance_df[importance_df$Importance > median_sensitivity,]
shortlisted_features <- shortlisted$Feature
shortlist_train <- X_train_processed[,shortlisted_features]
shortlist_test <- X_test_processed[, shortlisted_features]
final_model1 <- build_deepsurv(
num_input = ncol(shortlist_train),
num_layer = best_params$num_layer,
num_nodes = best_params$num_nodes,
string_activation = "selu",
num_l2 = best_params$num_l2,
num_dropout = best_params$num_dropout,
num_lr = best_params$num_lr,
lr_decay = 1e-2
)
predicted_log_risk_deepsurv <- final_model1 %>% predict(shortlist_test)
predicted_log_risk_deepsurv <- final_model1 %>% predict(shortlist_test); predicted_log_risk_deepsurv
predicted_relative_risk_deepsurv <- exp(predicted_log_risk_deepsurv); predicted_relative_risk_deepsurv
mean(predicted_relative_risk_deepsurv)
median(predicted_relative_risk_deepsurv)
cat("Predicted Risk Score for Patient 2:", predicted_relative_risk_deepsurv[2], "\n")
cat("Predicted Risk Score for Patient 10:", predicted_relative_risk_deepsurv[10], "\n") #observe event(progression) sooner
deepsurv_risks <- cbind(shortlist_test, predicted_log_risk_deepsurv, predicted_relative_risk_deepsurv)
deepsurv_risks
View(deepsurv_risks)
deepsurv_risks <- data.frame(shortlist_test, predicted_log_risk_deepsurv, predicted_relative_risk_deepsurv)
deepsurv_risks
View(deepsurv_risks)
#=====================Fivenumber Summary DeepSurv-risk====================
pred_df <- tibble(surv_risk = as.vector(predicted_relative_risk_deepsurv))
# Five-number summary + extras
fivenum_deepsurv_risk <- pred_df %>%
summarise(
n = n(),
mean = mean(surv_risk, na.rm = TRUE),
stdev = sd(surv_risk, na.rm = TRUE),
min = min(surv_risk, na.rm = TRUE),
q1 = quantile(surv_risk, 0.25, na.rm = TRUE),
median = median(surv_risk, na.rm = TRUE),
q3 = quantile(surv_risk, 0.75, na.rm = TRUE),
max = max(surv_risk, na.rm = TRUE)
)
print(fivenum_deepsurv_risk)
