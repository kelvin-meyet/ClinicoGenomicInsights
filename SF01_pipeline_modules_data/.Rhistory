)
write.csv(full_df, file = "D:/data_science/Forage/LBG - Project/full_df2.csv", row.names = F)
# # Final check
cat("ML Modelling Data Dimension: The combined dataset with onehot-encoded categorical
features comprised of", nrow(full_df), "observations", ncol(full_df), "features")
table(master_final$marital_status)
quarto check jupyter
# Set your Working Directory and load R source code of dependent libraries
# install.packages("rstudioapi")
library(rstudioapi)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
getwd()
source("pcm_libraries.R")
source("rsf_libraries.R")
source("deepsurv_libraries.R")
source("deepsurv.R")
#-----------------------> Load Data <----------------------------
#--- Read any of the data below to run the respective ML Experiment Pipeline--
# clinical <- read.csv("prca_clinicogenomics_data.csv")%>%
#   select(age:pfs_status)
clinical_all_genomic <- read.csv("prca_clinicogenomics_data.csv")
# clinical_novel_genomic <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")%>%
#   select(-c(fat3_diff, atm_diff, kmt2d_diff, foxa1_diff, tp53_diff, spop_diff,
#             smad4_diff,lrp1b_diff,  idh1_diff, ctnnb1_diff, braf_diff, kmt2c_diff))
# clinical_known_genomic <- read.csv("cleaned_imputed_data_for_ml-integrated.csv")%>%
#   select(-c(nkx3_1_diff, csmd3_diff, trrap_diff, chd4_diff, vwf_diff, ephb1_diff, herc2_diff, mcm3_diff,
#             spta1_diff, sall1_diff,  herc1_diff, rybp_diff, ttn_diff, chd5_diff, myh6_diff))
dat_new <- clinical_all_genomic  #----> update this based on the data options loaded
#----------------------->Data Modelling<-----------------
#---convert integer to numeric and character to categorical-
integer_cols <- sapply(dat_new, is.integer)
dat_new[integer_cols] <- lapply(dat_new[integer_cols], as.numeric)
# Convert character columns to categorical
char_cols <- sapply(dat_new, is.character)
dat_new[char_cols] <- lapply(dat_new[char_cols], factor)
#---Arrange data---
endpoints <- dat_new %>%  #we need status be integer & months be  but numeric here
dplyr::select(pfs_months, pfs_status)
vars <- dat_new %>%
dplyr::select(-c(pfs_months, pfs_status))
pfs_status <- endpoints$pfs_status
pfs_months <- endpoints$pfs_months
#---- Partition Data ----
set.seed(1)
comb_dat <- cbind(vars, pfs_months, pfs_status)
sample = sample.split( Y= comb_dat$pfs_status, SplitRatio = 0.70)
train <- subset(comb_dat, sample == TRUE)
test  <- subset(comb_dat, sample == FALSE)
#---Train set --
X_train <- train %>% select(-c(pfs_months, pfs_status))
Y_train <- train %>% select(pfs_months, pfs_status)
#-- Test set --
X_test <- test %>% select(-c(pfs_months, pfs_status))
Y_test <- test %>% select(pfs_months, pfs_status)
#---Process Train Data---
#One-Hot Encode Categorical Columns
catego_columns <- sapply(X_train, is.factor)
categorical_columns <- colnames(X_train)[catego_columns]
train_categorical <- train %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~ ., data = .) %>%
as.data.frame()
num_columns_train <- sapply(X_train, is.numeric)
numeric_columns_train <- X_train[num_columns_train]
#combine onehot & scaled
X_train_processed <- cbind(numeric_columns_train, train_categorical)
#--- Process Test Data ---
# -- one-hot encode categorical
test_categorical <- test %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~., data = ., xlev = attr(train_categorical, "contrasts")) %>%
as.data.frame()
num_columns_test <- sapply(X_test, is.numeric)
numeric_columns_test <- X_test[num_columns_test]
#combine one-hot & scaled test
X_test_processed <- cbind(numeric_columns_test, test_categorical)
dim(X_train_processed)
dim(X_test_processed)
dim(Y_train)
dim(Y_test)
processed_train_data <- data.frame(cbind(X_train_processed, Y_train))
processed_test_data <- data.frame(cbind(X_test_processed, Y_test))
X_train_processed <- as.matrix(X_train_processed)
Y_train <- as.matrix(Y_train)
X_test_processed <- as.matrix(X_test_processed)
Y_test <- as.matrix(Y_test)
# Predefined fold assignment
foldid <- sample(1:5, size = nrow(processed_train_data), replace = TRUE)
#================= Penalized COx Model ================
pfs_ytrain <- cbind(time=Y_train[, "pfs_months"], status=Y_train[, "pfs_status"])
pfs_ytest <- cbind(time=Y_test[, "pfs_months"], status=Y_test[, "pfs_status"])
# Run / loop over all aphas
alphas = c(0, 0.25, 0.5, 0.75, 1)
cv_all_alpha <- data.frame(iter      = integer(),
c_indexes = numeric(),
lambdas   = numeric(),
non_zer0_coef = integer(),
stringsAsFactors = FALSE)
for(i in alphas){
cv_0  <- cv.glmnet(x            = X_train_processed,
y            = pfs_ytrain,
foldid       = foldid,
alpha        = i,
type.measure = "C",
family       = "cox",
standardize  = TRUE,
relax        = TRUE,
trace.it     = 0)
alpha_parameter = i
c_index <- as.numeric(cv_0$cvm[cv_0$index[1,1]])
lambda = as.numeric(cv_0$lambda.min)
pred = as.numeric(cv_0$nzero[cv_0$index[1,1]])
cv_all_alpha = rbind(cv_all_alpha,
data.frame(alpha = alpha_parameter , c_indexes = c_index, lambdas = lambda, non_zero_coef = pred) )
}
print(cv_all_alpha)
#-- Final Fitted Model on Train data with optimal parameters ---
best_cv_metrics=cv_all_alpha[which.max(cv_all_alpha$c_indexes), ]
best_cv_metrics
bestlambda = best_cv_metrics$lambdas
bestalpha = best_cv_metrics$alpha
pcm_clinical_fit <- glmnet(x           = X_train_processed,
y           = pfs_ytrain,
family      = "cox",
lambda      = bestlambda,
alpha       = bestalpha,
intercept   = TRUE,
standardize = T)
#---selected variables---
vars_ridge <- coef(pcm_clinical_fit, s = bestlambda); vars_ridge
vars_ridge <- as.matrix(vars_ridge)
ridge_selected <- names(vars_ridge[vars_ridge != 0,])
selected_variables <- as.character(ridge_selected)
cat(selected_variables, sep = "\n")
coefs_clinical_2 <- as.data.frame.matrix((coef(pcm_clinical_fit, s = bestlambda))) %>%
filter(.[,1] != 0) %>%
arrange(desc((.[, 1]))) %>%
round(., 3)
# Replace spaces with periods in variable names
# Adjust variable names to avoid extra backticks
variable_names <- rownames(coefs_clinical_2)
variable_names <- sapply(variable_names, function(x) {
x <- gsub(" ", ".", x) # Replace spaces with periods
if (x %in% names(processed_train_data)) {
x  # Use name directly if it exists in the dataset
} else if (grepl("[^a-zA-Z0-9_]", x)) {
paste0("`", x, "`")  # Add backticks only when necessary
} else {
x
}
})
formula_final <- paste("Surv(pfs_months, pfs_status) ~", paste(variable_names, collapse = " + "))
surv_formula22 <- as.formula(formula_final)
# Print the updated formula
print(surv_formula22)
missing_vars <- setdiff(variable_names, names(processed_train_data))
print(missing_vars)
#---> Final Cox model
cox_model_final<-coxph(surv_formula22, data=processed_train_data, x=T, model = T)
summary(cox_model_final)
base_surv <- survfit(cox_model_final)
summary(base_surv)
baseline_df <- data.frame(
time = base_surv$time,
n_risk = base_surv$n.risk,
n_event = base_surv$n.event,
surv = base_surv$surv,
cumhaz = base_surv$cumhaz
)
base_surv$cumhaz # The estimated cumulative baseline hazard
base_surv$surv # The estimated baseline survival prob
# Concordant - Index on Train Data
surv_pred_ridge <- predict(pcm_clinical_fit, newx=X_train_processed, type ="response")
predicted_results<-assess.glmnet(surv_pred_ridge, newy = pfs_ytrain, family = "cox"); predicted_results
k=assess.glmnet(pcm_clinical_fit, newx = X_train_processed, newy =  pfs_ytrain, family = "cox")
pred_C <- k$C[1]
pred_C
#---Concordant - Index on Test Data ---
surv_pred_ridge <- predict(pcm_clinical_fit, newx=X_test_processed, type ="response")
predicted_results<-assess.glmnet(surv_pred_ridge, newy = pfs_ytest, family = "cox"); predicted_results
k=assess.glmnet(pcm_clinical_fit, newx = X_test_processed, newy =  pfs_ytest, family = "cox") ##test_surv_response
pred_C <- k$C[1]
pred_C
iterations <- 10
results_new <- data.frame()
for (iter in 1:iterations){
#-----------Training period------------------
cv_check <- cv.glmnet(x = X_train_processed,  #x_train,
y = pfs_ytrain, #train_surv_response,
#foldid = foldid, #gave constant preds
nfolds = 5,
alpha = bestalpha,
type.measure = "C",
family = "cox",
standardize = TRUE ) #default
alpha = bestalpha
BEST_LAMBDA <- round(cv_check$lambda.min, 4)
variables <-as.numeric(cv_check$nzero[cv_check$index[1,1]])
BEST_CONCORDANCE <- round(cv_check$cvm[as.numeric(cv_check$index[1,1])], 4)
SE = round(cv_check$cvsd[as.numeric(cv_check$index[1,1])], 4)
fit_r <- glmnet(x           = X_train_processed,
y           = pfs_ytrain,
family      = "cox",
lambda      = BEST_LAMBDA,
alpha       = bestalpha,
standardize = T)
lambdas_new = round(fit_r$lambda, 4) #fitted
#---------------------  Predict on test data -------------------------
surv_pred_clinical <- predict(fit_r, newx = X_test_processed, type = "link") #predictions
k <- round(as.numeric(assess.glmnet(surv_pred_clinical, newy = pfs_ytest, family = "cox")$C), 4)
results_new <- rbind(results_new,
data.frame(iteration = iter, penalty = alpha, sel_lambda = BEST_LAMBDA,
fitted_lambda = lambdas_new, sel_vars = variables, std.error = SE,
train_concordance = BEST_CONCORDANCE, Predicted_Concordance = k))
}
results_pred_clinical <- results_new
average_pred<-mean(results_pred_clinical$Predicted_Concordance)
cat("The average train concordance after ",iter," runs is: ",mean(results_pred_clinical$train_concordance))
cat("The sd train concordance after ",iter," runs is: ",sd(results_pred_clinical$train_concordance))
cat("The average predicted concordance after ",iter," runs is: ",mean(results_pred_clinical$Predicted_Concordance))
cat("The sd predicted concordance after ",iter," runs is: ",sd(results_pred_clinical$Predicted_Concordance))
#--Survival Predictions---predict at time = 72 months
library(survex)
#works for selected models coxph, rsf but dont see neural network models
explainer_cox <- explain(cox_model_final)
#------survival predictions--------
req_variable_names <- sapply(variable_names, function(x) {
x <- gsub(" ", ".", x) # Replace spaces with periods
if (x %in% names(processed_test_data)) {
x  # Use name directly if it exists in the dataset
} else if (grepl("[^a-zA-Z0-9_]", x)) {
paste0("`", x, "`")  # Add backticks only when necessary
} else {
x
}
})
X_test_variables <- unique(req_variable_names)
X_test_pred <- as.data.frame(X_test_processed)
# Ensure column names are formatted consistently (replace spaces with dots)
colnames(X_test_pred) <- gsub(" ", ".", colnames(X_test_pred))
# Subset only the required columns dynamically
X_test_pred_req <- X_test_pred[, X_test_variables, drop = FALSE]
pred_cox_surv <- predict(explainer_cox, newdata = X_test_pred_req, output_type="survival", times = 72) #max(test_data$pfs_months)
pred_cox_surv[c(2,10),]
mean(pred_cox_surv)
median(pred_cox_surv)
#=====================Fivenumber Summary PCM====================
pred_df <- tibble(surv_prob = as.vector(pred_cox_surv))
# Five-number summary + extras
fivenum_pcm_surv <- pred_df %>%
summarise(
n = n(),
mean = mean(surv_prob, na.rm = TRUE),
stdev = sd(surv_prob, na.rm = TRUE),
min = min(surv_prob, na.rm = TRUE),
q1 = quantile(surv_prob, 0.25, na.rm = TRUE),
median = median(surv_prob, na.rm = TRUE),
q3 = quantile(surv_prob, 0.75, na.rm = TRUE),
max = max(surv_prob, na.rm = TRUE)
)
print(fivenum_pcm_surv)
#risk--> does not matter time
pred_cox_risk <- predict(explainer_cox, newdata = X_test_pred_req, output_type="risk")
pred_cox_risk[c(2,10)]
mean(pred_cox_risk)
median(pred_cox_risk)
predictions_risk_matrix_cox <- cbind(X_test_pred_req, pred_cox_surv, pred_cox_risk)
predictions_risk_matrix_cox
#=====================Fivenumber Summary PCM====================
pred_df <- tibble(surv_prob = as.vector(pred_cox_risk))
# Five-number summary + extras
fivenum_pcm_risk <- pred_df %>%
summarise(
n = n(),
mean = mean(surv_prob, na.rm = TRUE),
stdev = sd(surv_prob, na.rm = TRUE),
min = min(surv_prob, na.rm = TRUE),
q1 = quantile(surv_prob, 0.25, na.rm = TRUE),
median = median(surv_prob, na.rm = TRUE),
q3 = quantile(surv_prob, 0.75, na.rm = TRUE),
max = max(surv_prob, na.rm = TRUE)
)
print(fivenum_pcm_risk)
#=======================================  Random Survival Forest =================================
#Prep Data for RSF ---> train & test are dataframes
#===Train set====
X_train <- train %>% select(-c(pfs_months, pfs_status))
Y_train <- train %>% select(pfs_months, pfs_status)
#===Test set===
X_test <- test %>% select(-c(pfs_months, pfs_status))
Y_test <- test %>% select(pfs_months, pfs_status)
#=====Process Train Data===
#One-Hot Encode Categorical Columns
catego_columns <- sapply(X_train, is.factor)
categorical_columns <- colnames(X_train)[catego_columns]
train_categorical <- train %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~ . -1, data = .) %>%    #Remove intercept term
as.data.frame() %>%
select(-hist_neoadjuv_trtmntNo)  #Remove redundant term
num_columns_train <- sapply(X_train, is.numeric)
numeric_columns_train <- X_train[num_columns_train]
#combine onehot & scaled
X_train_processed <- cbind(numeric_columns_train, train_categorical)
#===Process Test Data===
#one-hot encode categorical
test_categorical <- test %>%
select(all_of(categorical_columns)) %>%
mutate(across(everything(), as.factor)) %>%
model.matrix(~.-1, data = ., xlev = attr(train_categorical, "contrasts")) %>%
as.data.frame() %>%
select(-hist_neoadjuv_trtmntNo) #remove redundnt level
num_columns_test <- sapply(X_test, is.numeric)
numeric_columns_test <- X_test[num_columns_test]
#combine one-hot & scaled test
X_test_processed <- cbind(numeric_columns_test, test_categorical)
dim(X_train_processed)
dim(X_test_processed)
dim(Y_train)
dim(Y_test)
processed_train_data <- data.frame(cbind(X_train_processed, Y_train))
processed_test_data <- data.frame(cbind(X_test_processed, Y_test))
X_train_processed <- as.matrix(X_train_processed)
Y_train <- as.matrix(Y_train)
X_test_processed <- as.matrix(X_test_processed)
Y_test <- as.matrix(Y_test)
results <- data.frame(nodesize = numeric(),  mtry = numeric(), ntree = numeric(), error = numeric())
trees <- c(seq(10,100, by=10), 150, 200, 250, 300)
for (tree_val in trees) {
for (fold in unique(foldid)) {
train_data <- processed_train_data[foldid != fold, ]
test_data <- processed_train_data[foldid == fold, ]
# Tune a random survival forest model
tune0 <- tune(formula     = Surv(pfs_months, pfs_status) ~ .,
data        = train_data,
mtryStart   = 1,
nodesizeTry = c(1:9, seq(10, 40, by = 5)),
ntreeTry    = tree_val
)
opt          <- tune0$results[which.min(tune0$results[,3]), ] #optimal info
mtry_val     <- as.numeric(opt[2])   #fetch optimal mtry
nodesize_val <- as.numeric(opt[1])   # fetch optimal nodesize
ntree_val    <- as.numeric(tree_val) # fetch optimal tree
error_val    <- as.numeric(opt[3])   # fetch optimal cv error
results <- rbind(results, data.frame(mtry = mtry_val, nodesize = nodesize_val, ntree = tree_val, error = error_val))
}
}
rsf_clin_best_cv_results <- results
best_parameters_clin <- results[which.min(rsf_clin_best_cv_results$error), ]
best_parameters_clin
#=====================================================================
# ===============BUILD BEST RSF MODEL===========================
#==================================================================
rsf_clinical_fit <- rfsrc(formula        = Surv(pfs_months, pfs_status) ~ .,
data           = processed_train_data,
perf.type      = "default",
mtry           = best_parameters_clin$mtry,
nodesize       = best_parameters_clin$nodesize,
ntree          = best_parameters_clin$ntree,
importance     = TRUE,
forest         = TRUE
)
rsf_clinical_fit
#==Variable Importance Minimal Depth
rsf_vars<-var.select(rsf_clinical_fit, method = "md") #good
rsf_vars$topvars
#--survex---Survival Predictions---
#predict at time = 72 months
library(survex)
#works for selected models coxph, rsf but dont see neural network models
#Build explainer model with only selected variables
rsf_vars_req<- rsf_vars$topvars
X_train_pred <- as.data.frame(X_train_processed)
colnames(X_train_pred) <- gsub(" ", ".", colnames(X_train_pred))
X_train_pred_req <- X_train_pred[, rsf_vars_req]
#Final rsf model with selected variables
rsf_final_data <- cbind(X_train_pred_req, Y_train)
rsf_clinical_fit_final <- rfsrc(formula        = Surv(pfs_months, pfs_status) ~ .,
data           = rsf_final_data,
perf.type      = "default",
mtry           = best_parameters_clin$mtry,
nodesize       = best_parameters_clin$nodesize,
ntree          = best_parameters_clin$ntree,
importance     = TRUE,
forest         = TRUE
)
#==Explainer Created==
explainer_rsf <- explain(rsf_clinical_fit_final, data = X_train_pred_req)
#Build test data-matrix & Predict probabilities
X_test_pred_rsf <- as.data.frame(X_test_processed)
colnames(X_test_pred_rsf) <- gsub(" ", ".", colnames(X_test_pred_rsf))
X_test_pred_rsf_req <- X_test_pred_rsf[, rsf_vars_req]
pred_rsf <- predict(explainer_rsf, newdata = X_test_pred_rsf_req, output_type="survival", times = 72 ) #max(test_data$pfs_months)
pred_rsf[c(2,10),]
mean(pred_rsf)
median(pred_rsf)
#=====================Fivenumber Summary RSF-surv====================
pred_df <- tibble(surv_prob = as.vector(pred_rsf))
# Five-number summary + extras
fivenum_rsf_surv <- pred_df %>%
summarise(
n = n(),
mean = mean(surv_prob, na.rm = TRUE),
stdev = sd(surv_prob, na.rm = TRUE),
min = min(surv_prob, na.rm = TRUE),
q1 = quantile(surv_prob, 0.25, na.rm = TRUE),
median = median(surv_prob, na.rm = TRUE),
q3 = quantile(surv_prob, 0.75, na.rm = TRUE),
max = max(surv_prob, na.rm = TRUE)
)
print(fivenum_rsf_surv)
#risk--> does not matter time
pred_rsf_risk <- predict(explainer_rsf, newdata = X_test_pred_rsf_req, output_type="risk")
pred_rsf_risk[c(2,10)]
mean(pred_rsf_risk)
median(pred_rsf_risk)
predictions_risk_matrix_rsf <- cbind(X_test_pred_rsf_req, pred_rsf, pred_rsf_risk)
predictions_risk_matrix_rsf
#=====================Fivenumber Summary RSF-risk====================
pred_df <- tibble(surv_prob = as.vector(pred_rsf_risk))
# Five-number summary + extras
fivenum_rsf_risk <- pred_df %>%
summarise(
n = n(),
mean = mean(surv_prob, na.rm = TRUE),
stdev = sd(surv_prob, na.rm = TRUE),
min = min(surv_prob, na.rm = TRUE),
q1 = quantile(surv_prob, 0.25, na.rm = TRUE),
median = median(surv_prob, na.rm = TRUE),
q3 = quantile(surv_prob, 0.75, na.rm = TRUE),
max = max(surv_prob, na.rm = TRUE)
)
print(fivenum_rsf_risk)
#---------Prediction accuracy of best model on train data---------------
pred_rsf_clinical_fit_train <- predict(object = rsf_clinical_fit, newdata = processed_train_data)
pred_rsf_clinical_fit_train
# 1 - requested performance error = accuracy index
pred_rsf_clinical_fit_c_index_train <- 1 - get.cindex(time = pred_rsf_clinical_fit_train$yvar[,1], censoring =pred_rsf_clinical_fit_train$yvar[,2],
predicted = pred_rsf_clinical_fit_train$predicted)
pred_rsf_clinical_fit_c_index_train
#---------Prediction accuracy of best model on test data---------------
pred_rsf_clinical_fit <- predict(object = rsf_clinical_fit, newdata = processed_test_data)
pred_rsf_clinical_fit
# 1 - requested performance error = accuracy index
pred_rsf_clinical_fit_c_index2 <- 1 - get.cindex(time = pred_rsf_clinical_fit$yvar[,1], censoring =pred_rsf_clinical_fit$yvar[,2],
predicted = pred_rsf_clinical_fit$predicted)
pred_rsf_clinical_fit_c_index2
#-------------------------Predict on test set for 10 iterations------------------------------
# Initialize an empty data frame to store results
results_all <- data.frame(iteration = numeric(),
nodesize_best = numeric(),
mtry_best = numeric(),
ntree_best = numeric(),
train_concordance = numeric(),
pred_concordance = numeric())
iterations <- 10
for (iter in 1:iterations) {
# Initialize results data frame
par_results <- data.frame(nodesize = numeric(), mtry = numeric(), ntree = numeric(), train_error = numeric())
trees <- c(seq(10,100, by=10), 150, 200)
for (tree_val in trees) {
for (fold in unique(foldid)) {
train_data <- processed_train_data[foldid != fold, ]
test_data <- processed_train_data[foldid == fold, ]
tune0 <- tune(formula = Surv(pfs_months, pfs_status) ~ .,
data = train_data,
mtryStart = 1,
nodesizeTry = c(1:9, seq(10, 40, by = 5)),
ntreeTry = tree_val)
opt <- tune0$results[which.min(tune0$results[, 3]), ]
mtry_val <- as.numeric(opt[2])
nodesize_val <- as.numeric(opt[1])
ntree_val <- as.numeric(tree_val)
error_val <- as.numeric(opt[3])
par_results <- rbind(par_results,
data.frame(mtry = mtry_val, nodesize = nodesize_val, ntree = tree_val, train_error = error_val))
}
}
# Optimal parameters
best_par <- par_results[which.min(par_results$train_error), ]
#---final model with tuned hyper-parameters------
final_model <- rfsrc(formula = Surv(pfs_months, pfs_status) ~ .,
data = processed_train_data,
mtry = best_par$mtry,
nodesize = best_par$nodesize,
ntree = best_par$ntree)
# Predict on test data - model never seen & train data
rsf_pred <- predict(object = final_model, newdata = processed_test_data)
rsf_pred_train <- predict(object = final_model, newdata = processed_train_data)
train_pred <- rsf_pred_train$predicted
test_pred <- rsf_pred$predicted
c_index <- get.cindex(time = processed_test_data$pfs_months, censoring = processed_test_data$pfs_status,
predicted = rsf_pred$predicted)
c_index_train <- get.cindex(time = processed_train_data$pfs_months, censoring = processed_train_data$pfs_status,
predicted = rsf_pred_train$predicted)
# Append results to results_all
results_all <- rbind(results_all, data.frame(iteration = iter,
nodesize_best = best_par$nodesize,
mtry_best = best_par$mtry,
ntree_best = best_par$ntree,
train_concordance = 1 - c_index_train,  #1-best_par$train_error,
pred_concordance = 1 - c_index))
}
rsf_pred_results_clinical <- results_all
average_pred<-mean(rsf_pred_results_clinical$pred_concordance)
cat("The average train concordance after ",iter," runs is: ",  mean(rsf_pred_results_clinical$train_concordance))
cat("The sd of train concordance after ",iter," runs is: ",  sd(rsf_pred_results_clinical$train_concordance))
cat("The average test concordance after ",iter," runs is: ",  mean(rsf_pred_results_clinical$pred_concordance))
cat("The sd of test concordance after ",iter," runs is: ",  sd(rsf_pred_results_clinical$pred_concordance))
